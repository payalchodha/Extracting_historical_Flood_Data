{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape The Tribune India using BeautifulSoup\n",
    "\n",
    "https://www.tribuneindia.com/archive.aspx\n",
    "\n",
    "Extracting headline, date published, caption and url from a topic word search in India Tribune Archives from 2010 to 2019, for the five keywords/phrases below:\n",
    "\n",
    "\n",
    "1) flood\n",
    "\n",
    "2) flooded\n",
    "\n",
    "3) flooding\n",
    "\n",
    "4) monsoon\n",
    "\n",
    "5) heavy rain\n",
    "\n",
    "This was initally done in five seperate notebooks. \n",
    "\n",
    "# WARNING: this notebook will take a long time to run\n",
    "\n",
    "The methodology behind this process was to first, extract as much surface level information as possible about the articles listed in the resultant archive search for each word. Then, using the urls found, the text from each article was extracted and stored in a seperate dataframe. Next the two data frames were merged and the superfluous columns were dropped. The resultant CSV had three columns, url, date and text. \n",
    "\n",
    "These csv's are combined in tribune_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_df():\n",
    "    \n",
    "    #url is the first page that appears after clicking go when searching The Tribune India archives\n",
    "    url = 'https://www.tribuneindia.com/archive_search.aspx?txt=Flood&catid=2&fd=01/01/2010%2012:00:00%20AM&td=11/01/2019%2012:00:00%20AM'\n",
    "    res = requests.get(url)\n",
    "\n",
    "    print(res.status_code)\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "    d = {'headline': headlines, \n",
    "     'date': dates, \n",
    "     'caption': captions, \n",
    "     'url': urls}\n",
    "    \n",
    "    df_mother = pd.DataFrame(d)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #change range according to how many pages available in The Tribune India archive for specified topic\n",
    "    for i in range(2,16): \n",
    "        \n",
    "        url1 = url + '&page=' + str(i)\n",
    "        res = requests.get(url1)\n",
    "        \n",
    "        print(res.status_code)\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        \n",
    "        urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "        d2 = {'headline': headlines, \n",
    "             'date': dates, \n",
    "             'caption': captions, \n",
    "             'url': urls}\n",
    "    \n",
    "        df = pd.DataFrame(d2)\n",
    "\n",
    "        print(len(urls))\n",
    "        print(len(dates))\n",
    "        \n",
    "        df_mother = pd.concat([df_mother, df], ignore_index=True)\n",
    "    \n",
    "        time.sleep(2)\n",
    "        \n",
    "#     return(col1, col2)\n",
    "    return(df_mother)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = web_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(url_df))\n",
    "print(url_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('../data/trib_flood_url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/trib_flood_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scrape():\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    #range changes based on how many urls pulled\n",
    "    for i in range(301):\n",
    "        url_base = 'https://www.tribuneindia.com/'\n",
    "        url = url_base + df['url'][i]\n",
    "\n",
    "        res = requests.get(url)\n",
    "\n",
    "        print(res.status_code)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        content = [i.text.replace('\\n', ' ') for i in soup.find_all('span', {'class': 'storyText'})]\n",
    "        \n",
    "        content_dict.update({i: content})\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return content_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = content_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_flood_text = pd.DataFrame.from_dict(content_dict, orient='index')\n",
    "trib_flood_text.rename(columns={0: 'text', inplace=True})\n",
    "trib_flood_text.to_csv('../data/trib_flood_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flooded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_df():\n",
    "\n",
    "    url = 'https://www.tribuneindia.com/archive_search.aspx?txt=Flooded&catid=2&fd=01/01/2010%2012:00:00%20AM&td=11/01/2019%2012:00:00%20AM'\n",
    "    res = requests.get(url)\n",
    "\n",
    "    res.status_code\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "    d = {'headline': headlines, \n",
    "     'date': dates, \n",
    "     'caption': captions, \n",
    "     'url': urls}\n",
    "    \n",
    "    df_mother = pd.DataFrame(d)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    #change range according to how many pages available in The Tribune India archive for specified topic\n",
    "    for i in range(2,12): \n",
    "        \n",
    "        url1 = url + '&page=' + str(i)\n",
    "        res = requests.get(url1)\n",
    "        \n",
    "        print(res.status_code)\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        \n",
    "        urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "        d2 = {'headline': headlines, \n",
    "             'date': dates, \n",
    "             'caption': captions, \n",
    "             'url': urls}\n",
    "    \n",
    "        df = pd.DataFrame(d2)\n",
    "\n",
    "        print(len(urls))\n",
    "        print(len(dates))\n",
    "        \n",
    "        df_mother = pd.concat([df_mother, df], ignore_index=True)\n",
    "    \n",
    "        time.sleep(2)\n",
    "\n",
    "    return(df_mother)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = web_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(url_df))\n",
    "print(url_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('../data/trib_flooded_url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/trib_flooded_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scrape():\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    #range changes based on how many urls pulled\n",
    "    for i in range(232):\n",
    "        url_base = 'https://www.tribuneindia.com/'\n",
    "        url = url_base + df['url'][i]\n",
    "\n",
    "        res = requests.get(url)\n",
    "\n",
    "        print(res.status_code)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        content = [i.text.replace('\\n', ' ') for i in soup.find_all('span', {'class': 'storyText'})]\n",
    "        \n",
    "        content_dict.update({i: content})\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return content_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = content_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_flooded_text = pd.DataFrame.from_dict(content_dict, orient='index')\n",
    "trib_flooded_text.rename(columns={0: 'text'}, inplace=True)\n",
    "trib_flooded_text.to_csv('../data/trib_flooded_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flooding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_df():\n",
    "\n",
    "    url = 'https://www.tribuneindia.com/archive_search.aspx?txt=Flooding&catid=2&fd=01/01/2010%2012:00:00%20AM&td=11/01/2019%2012:00:00%20AM'\n",
    "    res = requests.get(url)\n",
    "\n",
    "    res.status_code\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "    d = {'headline': headlines, \n",
    "     'date': dates, \n",
    "     'caption': captions, \n",
    "     'url': urls}\n",
    "    \n",
    "    df_mother = pd.DataFrame(d)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(2,6): \n",
    "        \n",
    "        url1 = url + '&page=' + str(i)\n",
    "        res = requests.get(url1)\n",
    "        \n",
    "        print(res.status_code)\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        \n",
    "        urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "        d2 = {'headline': headlines, \n",
    "             'date': dates, \n",
    "             'caption': captions, \n",
    "             'url': urls}\n",
    "    \n",
    "        df = pd.DataFrame(d2)\n",
    "\n",
    "        print(len(urls))\n",
    "        print(len(dates))\n",
    "        \n",
    "        df_mother = pd.concat([df_mother, df], ignore_index=True)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "    return(df_mother)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = web_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(url_df))\n",
    "print(url_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('../data/trib_flooding_url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/trib_flooding_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scrape():\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    for i in range(120):\n",
    "        url_base = 'https://www.tribuneindia.com/'\n",
    "        url = url_base + df['url'][i]\n",
    "\n",
    "        res = requests.get(url)\n",
    "\n",
    "        print(res.status_code)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        content = [i.text.replace('\\n', ' ') for i in soup.find_all('span', {'class': 'storyText'})]\n",
    "        \n",
    "        content_dict.update({i: content})\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return content_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = content_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(content_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_flooded_text = pd.DataFrame.from_dict(content_dict, orient='index')\n",
    "trib_flooded_text.rename(columns={0: 'text'}, inplace=True)\n",
    "trib_flooded_text.to_csv('../data/trib_flooding_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heavy Rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_df():\n",
    "\n",
    "    url = 'https://www.tribuneindia.com/archive_search.aspx?txt=heavy%20rain&catid=2&fd=01/01/2010%2012:00:00%20AM&td=11/01/2019%2012:00:00%20AM'\n",
    "    res = requests.get(url)\n",
    "\n",
    "    res.status_code\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "    d = {'headline': headlines, \n",
    "     'date': dates, \n",
    "     'caption': captions, \n",
    "     'url': urls}\n",
    "    \n",
    "    df_mother = pd.DataFrame(d)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(2,7): \n",
    "        \n",
    "        url1 = url + '&page=' + str(i)\n",
    "        res = requests.get(url1)\n",
    "        \n",
    "        print(res.status_code)\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        \n",
    "        urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "        d2 = {'headline': headlines, \n",
    "             'date': dates, \n",
    "             'caption': captions, \n",
    "             'url': urls}\n",
    "    \n",
    "        df = pd.DataFrame(d2)\n",
    "\n",
    "        print(len(urls))\n",
    "        print(len(dates))\n",
    "        \n",
    "        df_mother = pd.concat([df_mother, df], ignore_index=True)\n",
    "        \n",
    "        time.sleep(2)\n",
    "        \n",
    "    return(df_mother)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = web_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(url_df))\n",
    "print(url_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('../data/trib_heavy_rain_url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/trib_heavy_rain_url.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scrape():\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    for i in range(118):\n",
    "        url_base = 'https://www.tribuneindia.com/'\n",
    "        url = url_base + df['url'][i]\n",
    "\n",
    "        res = requests.get(url)\n",
    "\n",
    "        print(res.status_code)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        content = [i.text.replace('\\n', ' ') for i in soup.find_all('span', {'class': 'storyText'})]\n",
    "        \n",
    "        content_dict.update({i: content})\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return content_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = content_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_heavy_rain_text = pd.DataFrame.from_dict(content_dict, orient='index')\n",
    "trib_heavy_rain_text.rename(columns={0: 'text'}, inplace=True)\n",
    "trib_heavy_rain_text.drop(columns=[1,2,3,4], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_heavy_rain_text.to_csv('../data/trib_heavy_rain_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monsoon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_df():\n",
    "\n",
    "    url = 'https://www.tribuneindia.com/archive_search.aspx?txt=Monsoon&catid=2&fd=01/01/2010%2012:00:00%20AM&td=11/01/2019%2012:00:00%20AM'\n",
    "    res = requests.get(url)\n",
    "\n",
    "    res.status_code\n",
    "    soup = BeautifulSoup(res.content, 'lxml')\n",
    "    \n",
    "    urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "    d = {'headline': headlines, \n",
    "     'date': dates, \n",
    "     'caption': captions, \n",
    "     'url': urls}\n",
    "    \n",
    "    df_mother = pd.DataFrame(d)\n",
    "    \n",
    "    time.sleep(2)\n",
    "    \n",
    "    for i in range(2,18): \n",
    "        \n",
    "        url1 = url + '&page=' + str(i)\n",
    "        res = requests.get(url1)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        print(res.status_code)\n",
    "        \n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        \n",
    "        urls = [i('a', href=True)[0]['href'] for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        headlines = [i('a')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        dates = [i('span')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "        captions = [i('p')[0].text for i in soup.find_all('div', {'class': 'listing'})]\n",
    "    \n",
    "        d2 = {'headline': headlines, \n",
    "             'date': dates, \n",
    "             'caption': captions, \n",
    "             'url': urls}\n",
    "    \n",
    "        df = pd.DataFrame(d2)\n",
    "\n",
    "        print(len(urls))\n",
    "        print(len(dates))\n",
    "        \n",
    "        df_mother = pd.concat([df_mother, df], ignore_index=True)\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    return(df_mother)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df = web_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(url_df))\n",
    "print(url_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df.to_csv('../data/trib_monsoon_url.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/trib_monsoon_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_scrape():\n",
    "    \n",
    "    content_dict = {}\n",
    "    \n",
    "    for i in range(323):\n",
    "        url_base = 'https://www.tribuneindia.com/'\n",
    "        url = url_base + df['url'][i]\n",
    "\n",
    "        res = requests.get(url)\n",
    "\n",
    "        print(res.status_code)\n",
    "        soup = BeautifulSoup(res.content, 'lxml')\n",
    "        content = [i.text.replace('\\n', ' ') for i in soup.find_all('span', {'class': 'storyText'})]\n",
    "        \n",
    "        content_dict.update({i: content})\n",
    "        \n",
    "        time.sleep(3)\n",
    "    \n",
    "    return content_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_dict = content_scrape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trib_monsoon_text = pd.DataFrame.from_dict(content_dict, orient='index')\n",
    "trib_monsoon_text.drop(columns=[1], inplace=True)\n",
    "trib_monsoon_text.rename(columns={0:'text'}, inplace=True)\n",
    "trib_monsoon_text.to_csv('../data/trib_monsoon_text.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
