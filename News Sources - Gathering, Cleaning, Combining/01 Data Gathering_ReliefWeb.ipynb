{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data  \n",
    "---\n",
    "### Pulling articles from  [reliefweb.int](https://reliefweb.int/)\n",
    "Used the search \"Punjab flood\" with advanced search of  `primary country` filled in as \"India\". \n",
    "\n",
    "Reliefweb calls itself the \"leading online source for reliable and timely humanitarian information on global crises and disasters since 1996.\" Reliefweb is a digital service of the Coordination of Humanitarian Affairs (OCHA) of the United Nations. It monitors and gathers information from 4,000 global sources which include various international papers, government statments, social media posts, and humanitarian voluneteer organizations. \n",
    "\n",
    "Aid referenced: [link](https://www.geeksforgeeks.org/get-post-requests-using-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reliefweb search converter, found [here](https://reliefweb.github.io/search-converter/), was used to convert the search query of \"(punjab flood) AND primary_country.id:119\" to the url used for the request.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url of search\n",
    "url = \"https://api.reliefweb.int/v1/reports?appname=apidoc&limit=1000&profile=list&preset=latest&slim=1&query[value]=(punjab%20flood)%20AND%20primary_country.id%3A119&query[operator]=AND\"\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to json type\n",
    "jsondata = res.json()\n",
    "data = jsondata['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON is a data type which stands for JavaScript Object Notation. The raw response is converted to JSON because it can then be converted to dictionary objects to easily be used in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list called urls with the url which came up from the search \n",
    "urls = [ item['fields']['url'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check length of the urls (there should be 427 since that was how many articles were pulled )\n",
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling the articles from the urls from the api search \n",
    "# only pulling the title, date, and body of each article \n",
    "articles = []\n",
    "\n",
    "for url in urls:\n",
    "    data = {}\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, \"lxml\")\n",
    "\n",
    "    data['title'] = soup.find(\"h1\", {\"class\": \"node-title clearfix\"}).text\n",
    "    data['date'] = soup.find(\"span\", {\"class\": \"date-display-single\"}).text\n",
    "    data['body'] = soup.find(\"div\", {\"class\": \"field body\"}).text\n",
    "    \n",
    "    articles.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of articles pulled from above\n",
    "articles_df= pd.DataFrame(articles)\n",
    "# add the urls connected with each article\n",
    "articles_df['url'] = urls\n",
    "# turn the dataframe into a csv file\n",
    "articles_df.to_csv(\"./reliefweb_floods.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "articles_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports to change date which is a string into a datetime type\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append a column with datetime of when the article was released \n",
    "articles_df['datetime'] = articles_df['date'].apply( lambda x: datetime.strptime(x,'%d %b %Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "datetime(2011, 7, 2, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask for articles starting with jan 1, 2007 and after\n",
    "is_after_2006 = articles_df[\"datetime\"] >= datetime(2007, 1, 1, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe of articles after 2006\n",
    "articles_after06_df = articles_df.loc[is_after_2006, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_after06_df.to_csv(\"../data/reliefweb_floods_after06.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary** In this notebook, articles were pulled from the reliefweb.int website related to \"punjab flood\" with the advanced search of `primary country` set to \"India\". This search resulted in 427 urls which were then scrapped using BeautifulSoup to grab the titles, dates of publication, and body of text. This was converted to a csv file named reliefweb_floods.csv. Another csv file was created excluding all articles published before January 1, 2007. This was saved as a csv file named reliefweb_floods_after06.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
